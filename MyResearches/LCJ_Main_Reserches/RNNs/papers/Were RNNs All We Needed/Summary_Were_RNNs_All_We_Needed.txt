Were RNNs All We Needed
以下是這篇論文的摘要:

這篇論文重新審視了十多年前的循環神經網絡(RNN)模型:LSTM和GRU。主要觀點如下:

1. 通過移除LSTM和GRU中隱藏狀態對輸入、遺忘和更新門的依賴,這些模型可以使用並行掃描算法進行高效訓練,不再需要通過時間反向傳播(BPTT)。[1][2][3](段)

2. 作者提出了LSTM和GRU的簡化版本:minLSTM和minGRU。這些版本:
   - 使用的參數比原始版本少得多
   - 可以完全並行化訓練(對於長度為512的序列,速度提高了175倍)
   - 移除了輸出範圍的限制(如tanh激活函數)
   - 確保輸出在時間尺度上是獨立的[4][5][6][7]

3. 實驗表明,這些簡化版的RNN在多項任務上的表現可以媲美最新的序列模型,如Mamba和Transformer:
   - 在選擇性複製任務中,minLSTM和minGRU能夠解決該任務,而許多其他模型失敗
   - 在強化學習任務中,它們的性能與Decision Transformer、Aaren和Mamba相當
   - 在語言建模任務中,它們達到了與Mamba和Transformer相當的測試損失[10][11][12][13]

4. 作者質疑:"RNN是否就是我們一直需要的?"這暗示了簡單的RNN架構可能比我們想像的更強大,可能不需要更複雜的模型。[14]

5. 論文還討論了這些簡化RNN與最近提出的其他循環序列模型之間的相似性,指出許多這些模型本質上都可以用相似的並行掃描算法來訓練。[15][16]

總的來說,這項研究重新評估了經典RNN模型的潛力,提出了高效的簡化版本,並展示了它們與最新模型相當的性能,為序列建模領域提供了新的思路。