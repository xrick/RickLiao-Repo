@article{7ad66cba3b7e3abae7ef33122588512a146f7f77,
title = {A Survey on Multi-Task Learning},
year = {2017},
url = {https://www.semanticscholar.org/paper/7ad66cba3b7e3abae7ef33122588512a146f7f77},
abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
author = {Yu Zhang and Qiang Yang},
journal = {IEEE Transactions on Knowledge and Data Engineering},
volume = {34},
pages = {5586-5609},
doi = {10.1109/TKDE.2021.3070203},
arxivid = {1707.08114},
}

@article{25b1105d49eb5df613a0e01c5fe3fdab289a0c92,
title = {Feature-Based Transfer Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/25b1105d49eb5df613a0e01c5fe3fdab289a0c92},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
journal = {Transfer Learning},
volume = {null},
pages = {null},
doi = {10.1017/9781139061773.005},
}

@article{f2d73f72eb1a09e2b7d4f5027a89728e6e7b9f1d,
title = {AutoTL: Learning to Transfer Automatically},
year = {2020},
url = {https://www.semanticscholar.org/paper/f2d73f72eb1a09e2b7d4f5027a89728e6e7b9f1d},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
journal = {Transfer Learning},
volume = {null},
pages = {null},
doi = {10.1017/9781139061773.014},
}

@article{68e3762509e92e9b3337693b0a154757b6069a13,
title = {Model-Based Transfer Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/68e3762509e92e9b3337693b0a154757b6069a13},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
journal = {Transfer Learning},
volume = {null},
pages = {null},
doi = {10.1017/9781139061773.006},
}

@article{20aaaad3d6c22bf5e08262da116bb0e6d89e61b2,
title = {Transfer Learning in Reinforcement Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/20aaaad3d6c22bf5e08262da116bb0e6d89e61b2},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
journal = {Transfer Learning},
volume = {null},
pages = {null},
doi = {10.1017/9781139061773.010},
}

@article{0fdcfad7a20486517cfba422f280d07fb456798c,
title = {Privacy-Preserving Transfer Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/0fdcfad7a20486517cfba422f280d07fb456798c},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
doi = {10.1017/9781139061773.018},
}

@article{670a4ee6d1e150f21912d5172bfaf267c77dd3ff,
title = {Transfer Learning in Dialogue Systems},
year = {2020},
url = {https://www.semanticscholar.org/paper/670a4ee6d1e150f21912d5172bfaf267c77dd3ff},
abstract = {null},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
doi = {10.1017/9781139061773.021},
}

@article{96d278fa72d8a0d8c5d07402e90af6a79159d038,
title = {Transfer Learning in Recommender Systems},
year = {2020},
url = {https://www.semanticscholar.org/paper/96d278fa72d8a0d8c5d07402e90af6a79159d038},
abstract = {Recommendation systems are becoming more and more important nowadays providing companies with a great advantage. Data sparseness, though, is a major problem for collaborative filtering (CF) techniques in recommender systems, especially for new users and items, making it difficult to make good recommendations. That is where Transfer Learning comes in. In this study, the goal is to improve the quality of recommendations in a sparse dataset by exploiting knowledge from another dataset. More specifically, the goal is to improve the recommendations in the CDs & Vinyl domain by utilizing rating data from the Digital Music domain. Experimental results show that CDs & Vinyl recommendations can indeed benefit from making use of Digital Music recommendations},
author = {Qiang Yang and Yu Zhang and Wenyuan Dai and Sinno Jialin Pan},
journal = {Transfer Learning},
volume = {null},
pages = {null},
doi = {10.1017/9781139061773.022},
}

@article{157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3,
title = {Transfer Learning in Natural Language Processing},
year = {2019},
url = {https://www.semanticscholar.org/paper/157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3},
abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.},
author = {Sebastian Ruder and Matthew E. Peters and Swabha Swayamdipta and Thomas Wolf},
doi = {10.18653/v1/N19-5004},
}

@article{46d55d5f6d501d2b7600e5973e90ace9b38bf6da,
title = {Heterogeneous transfer learning},
year = {2017},
url = {https://www.semanticscholar.org/paper/46d55d5f6d501d2b7600e5973e90ace9b38bf6da},
abstract = {null},
author = {Ying Wei},
doi = {10.14711/thesis-991012554867303412},
}
