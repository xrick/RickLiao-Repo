@article{944e1a7b2c5c62e952418d7684e3cade89c76f87,
title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
year = {2005},
url = {https://www.semanticscholar.org/paper/944e1a7b2c5c62e952418d7684e3cade89c76f87},
abstract = {One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.},
author = {R. Ando and Tong Zhang},
journal = {J. Mach. Learn. Res.},
volume = {6},
pages = {1817-1853},
}

@article{727e1e16ede6eaad241bad11c525da07b154c688,
title = {A Model of Inductive Bias Learning},
year = {2000},
url = {https://www.semanticscholar.org/paper/727e1e16ede6eaad241bad11c525da07b154c688},
abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
author = {Jonathan Baxter},
journal = {ArXiv},
volume = {abs/1106.0245},
pages = {null},
doi = {10.1613/jair.731},
arxivid = {1106.0245},
}

@article{895217d527de919dfdfbfeae5362bf5adba984ce,
title = {A Dirty Model for Multi-task Learning},
year = {2010},
url = {https://www.semanticscholar.org/paper/895217d527de919dfdfbfeae5362bf5adba984ce},
abstract = {We consider multi-task learning in the setting of multiple linear regression, and where some relevant features could be shared across the tasks. Recent research has studied the use of l1/lq norm block-regularizations with q > 1 for such block-sparse structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the extent to which the features are shared across tasks. Indeed they show [8] that if the extent of overlap is less than a threshold, or even if parameter values in the shared features are highly uneven, then block l1/lq regularization could actually perform worse than simple separate elementwise l1 regularization. Since these caveats depend on the unknown true parameters, we might not know when and which method to apply. Even otherwise, we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well. 
 
Here, we ask the question: can we leverage parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such dirty data which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). With the explosion of such dirty high-dimensional data in modern settings, it is vital to develop tools - dirty models - to perform biased statistical estimation tailored to such data. Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we estimate a superposition of two sets of parameters and regularize them differently. We show both theoretically and empirically, our method strictly and noticeably outperforms both l1 or l1/lq methods, under high-dimensional scaling and over the entire range of possible overlaps (except at boundary cases, where we match the best method).},
author = {A. Jalali and Pradeep Ravikumar and S. Sanghavi and Chao Ruan},
}

@article{d2d35fc47bfcd9bbcdf1905b23be6e5dcee05e9c,
title = {Learning with Whom to Share in Multi-task Feature Learning},
year = {2011},
url = {https://www.semanticscholar.org/paper/d2d35fc47bfcd9bbcdf1905b23be6e5dcee05e9c},
abstract = {In multi-task learning (MTL), multiple tasks are learnt jointly. A major assumption for this paradigm is that all those tasks are indeed related so that the joint training is appropriate and beneficial. In this paper, we study the problem of multi-task learning of shared feature representations among tasks, while simultaneously determining "with whom" each task should share. We formulate the problem as a mixed integer programming and provide an alternating minimization technique to solve the optimization problem of jointly identifying grouping structures and parameters. The algorithm mono-tonically decreases the objective function and converges to a local optimum. Compared to the standard MTL paradigm where all tasks are in a single group, our algorithm improves its performance with statistical significance for three out of the four datasets we have studied. We also demonstrate its advantage over other task grouping techniques investigated in literature.},
author = {Z. Kang and K. Grauman and Fei Sha},
}

@article{1248ec7fae6c2b34a40cc0b99100227af6d2e980,
title = {Integrating low-rank and group-sparse structures for robust multi-task learning},
year = {2011},
url = {https://www.semanticscholar.org/paper/1248ec7fae6c2b34a40cc0b99100227af6d2e980},
abstract = {Multi-task learning (MTL) aims at improving the generalization performance by utilizing the intrinsic relationships among multiple related tasks. A key assumption in most MTL algorithms is that all tasks are related, which, however, may not be the case in many real-world applications. In this paper, we propose a robust multi-task learning (RMTL) algorithm which learns multiple tasks simultaneously as well as identifies the irrelevant (outlier) tasks. Specifically, the proposed RMTL algorithm captures the task relationships using a low-rank structure, and simultaneously identifies the outlier tasks using a group-sparse structure. The proposed RMTL algorithm is formulated as a non-smooth convex (unconstrained) optimization problem. We propose to adopt the accelerated proximal method (APM) for solving such an optimization problem. The key component in APM is the computation of the proximal operator, which can be shown to admit an analytic solution. We also theoretically analyze the effectiveness of the RMTL algorithm. In particular, we derive a key property of the optimal solution to RMTL; moreover, based on this key property, we establish a theoretical bound for characterizing the learning performance of RMTL. Our experimental results on benchmark data sets demonstrate the effectiveness and efficiency of the proposed algorithm.},
author = {Jianhui Chen and Jiayu Zhou and Jieping Ye},
doi = {10.1145/2020408.2020423},
}

@article{f500b1a7df00f67c417673e0538d86abb8a333fa,
title = {Facial Landmark Detection by Deep Multi-task Learning},
year = {2014},
url = {https://www.semanticscholar.org/paper/f500b1a7df00f67c417673e0538d86abb8a333fa},
abstract = {S2 TL;DR: A novel tasks-constrained deep model is formulated, with task-wise early stopping to facilitate learning convergence and reduces model complexity drastically compared to the state-of-the-art method based on cascaded deep model.},
author = {Zhanpeng Zhang and Ping Luo and Chen Change Loy and Xiaoou Tang},
doi = {10.1007/978-3-319-10599-4_7},
}

@article{e7fb16842536659580b839e5a859bafe20cb0dd8,
title = {Multi-Task Learning using Generalized t Process},
year = {2010},
url = {https://www.semanticscholar.org/paper/e7fb16842536659580b839e5a859bafe20cb0dd8},
abstract = {Multi-task learning seeks to improve the generalization performance of a learning task with the help of other related learning tasks. Among the multi-task learning methods proposed thus far, Bonilla et al.’s method (Bonilla et al., 2008) provides a novel multi-task extension of Gaussian process (GP) by using a task covariance matrix to model the relationships between tasks. However, learning the task covariance matrix directly has both computational and representational drawbacks. In this paper, we propose a Bayesian extension by modeling the task covariance matrix as a random matrix with an inverse-Wishart prior and integrating it out to achieve Bayesian model averaging. To make the computation feasible, we first give an alternative weight-space view of Bonilla et al.’s multi-task GP model and then integrate out the task covariance matrix in the model, leading to a multi-task generalized t process (MTGTP). For the likelihood, we use a generalized t noise model which, together with the generalized t process prior, brings about the robustness advantage as well as an analytical form for the marginal likelihood. In order to specify the inverse-Wishart prior, we use the maximum mean discrepancy (MMD) statistic to estimate the parameter matrix of the inverse-Wishart prior. Moreover, we investigate some theoretical properties of MTGTP, such as its asymptotic analysis and learning curve. Comparative experimental studies on two common multi-task learning applications show very promising results. Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&CP 9. Copyright 2010 by the authors.},
author = {Yu Zhang and D. Yeung},
}

@article{d82857f43bac0daa909479807f340995503fd7a4,
title = {Learning incoherent sparse and low-rank patterns from multiple tasks},
year = {2010},
url = {https://www.semanticscholar.org/paper/d82857f43bac0daa909479807f340995503fd7a4},
abstract = {We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multi-task learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is non-convex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is non-differentiable and the feasible domain is non-trivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem. In addition, we present two projected gradient algorithms and discuss their rates of convergence. Experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms.},
author = {Jianhui Chen and Ji Liu and Jieping Ye},
journal = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
volume = {null},
pages = {null},
doi = {10.1145/1835804.1835952},
}

@article{e3cd36c092abd65d6ac8e648f3468eeee90ee1fc,
title = {Learning from hints in neural networks},
year = {1990},
url = {https://www.semanticscholar.org/paper/e3cd36c092abd65d6ac8e648f3468eeee90ee1fc},
abstract = {S2 TL;DR: A method for incorporating any invariance hint about ƒ in any descent method for learning from examples is introduced and it is shown that learning in a neural network remains NP-complete with a certain, biologically plausible, hint about the network.},
author = {Y. Abu-Mostafa},
journal = {J. Complex.},
volume = {6},
pages = {192-198},
doi = {10.1016/0885-064X(90)90006-Y},
}

@article{dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8,
title = {Symbolic-Neural Systems and the Use of Hints for Developing Complex Systems},
year = {1991},
url = {https://www.semanticscholar.org/paper/dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8},
abstract = {S2 TL;DR: Two methods are investigated for adding knowledge in neural network systems: decomposition of networks; and rule-injection hints, which play a role similar to adding rules or defining algorithms in symbolic systems.},
author = {S. Suddarth and A. Holden},
journal = {Int. J. Man Mach. Stud.},
volume = {35},
pages = {291-311},
doi = {10.1016/S0020-7373(05)80130-0},
}
