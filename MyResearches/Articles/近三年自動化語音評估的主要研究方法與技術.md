<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 列出近三年自動化語音評估的主要研究方法與技術差異

近三年自動化語音評估的研究方法與技術有顯著變化，主要圍繞深度學習架構的革新、個人化模型應用、訓練資料多樣化，以及自監督學習與多語言適應等。下表匯整主要技術路線及差異：[^1][^2][^3][^4][^5][^6]

***

### 近三年主要研究方法與技術差異

| 技術/方法 | 核心特色與差異 | 代表論文/技術 | 說明摘要 |
| :-- | :-- | :-- | :-- |
| End-to-End 深度學習(Transformer, Conformer) | 直接映射聲學特徵至文本，簡化流程，增強語音到文字準確性 | arXiv: Automatic Speech Recognition in the Modern Era [^4] | E2E模型結合Transformer與Conformer架構；大量自動特徵學習取代傳統手工特徵。 |
| 自監督學習(SSL) | 利用大量非標註語音預訓練，顯著減少人工語音轉寫需求 | wav2vec 2.0, SAP Challenge [^5][^4] | SSL類技術如wav2vec 2.0提升低資源語言與異常語音識別效率。 |
| 個人化/分群建模 | 針對語音障礙、口音、族群差異分群訓練或個人化模型，顯著提升辨識差異性語音 | ASHA Journal, Green et al. [^3] | 為每位語障者建一模型，較群眾模型顯著降低錯誤率，特別在非典型語音上差異明顯。 |
| 雙模資料集設計 | 結合「朗讀語音」與「自然對話」訓練，解決口語模式間準確率斷層 | PMC, Xue et al. [^2][^3] | 增加對話語料訓練，針對語音障礙者辨識性能有明顯提升。 |
| 多語言/多方言適應 | 支援多語種、多方言語音，提升模型泛化能力，矯正語言分布不均問題 | Speech Accessibility Project [^5][^2] | 利用大型多語種語音集訓練，針對低資源語言的辨識也有重大進展。 |
| 評分指標/臨床比較 | 導入臨床分數（如PCC、*z*分數）、自動語音評分與臨床專家人工評分之間的交叉驗證 | JMIR, Ahn et al. [^1] | 融合ASR分數與臨床治療師評分，比較兩者的一致性並分析模型不足。 |


***

### 核心技術差異說明

- **End-to-End深度學習模型（Transformer/Conformer）**已成主流架構，具備連貫、無縫資料流、直接聲學到文字映射功能。[^4]
- **自監督學習（SSL）**革新資料利用與低資源語音識別，成為解決語音評估資料瓶頸的關鍵路徑。[^5][^4]
- **個人化及分群建模**針對多樣化口音、語音障礙、方言進行模型優化，顯著提升識別率，尤其對臨床應用造成深遠影響。[^2][^3]
- **多語言及自然對話語音資料集設計**成為提升模型泛化及適用性的主流趨勢。[^2][^5]
- **臨床指標引入與專家評分比較**則促進模型解釋性、信任度與實務落地。[^1]

研究方法與技術的快速演進，驅動自動化語音評估技術從單一標準語音走向多樣化、臨床導向與泛化能力強的模型設計。[^3][^6][^4][^5][^1][^2]
<span style="display:none">[^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^7][^8][^9]</span>

<div align="center">⁂</div>

[^1]: https://www.jmir.org/2025/1/e60520/

[^2]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12345943/

[^3]: https://pubs.asha.org/doi/10.1044/2024_JSLHR-24-00045

[^4]: https://arxiv.org/html/2510.12827v1

[^5]: https://arxiv.org/html/2507.22047v1

[^6]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12524231/

[^7]: https://www.tandfonline.com/doi/full/10.1080/2331186X.2025.2466288

[^8]: https://www.sciencedirect.com/science/article/pii/S2666307424000573

[^9]: https://www.cambridge.org/core/journals/language-teaching/article/automatic-speech-recognition-and-pronunciation-learning/E399449253AFD9E7E3E9D65579023A9F

[^10]: https://journal.code4lib.org/articles/17820

[^11]: https://www.nature.com/articles/s41598-025-96312-z

[^12]: https://opencv.org/blog/applications-of-speech-recognition/

[^13]: https://www.devopsschool.com/blog/top-10-speech-recognition-tools-in-2025-features-pros-cons-comparison-2/

[^14]: https://aclanthology.org/2023.bionlp-1.6/

[^15]: https://www.sciencedirect.com/science/article/abs/pii/S0885230823000578

[^16]: https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1210187/full

[^17]: https://aclanthology.org/2025.aaas-1.0.pdf

[^18]: https://www.datainsightsmarket.com/reports/speech-recognition-ai-1964707

[^19]: https://www.3playmedia.com/events/wbnr-05-01-2025-state-of-asr/

