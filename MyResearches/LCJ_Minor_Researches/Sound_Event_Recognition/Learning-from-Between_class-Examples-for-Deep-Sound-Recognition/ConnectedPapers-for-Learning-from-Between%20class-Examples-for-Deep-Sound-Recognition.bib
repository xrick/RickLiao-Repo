@article{2c5174a4f3f6978cc28e2d3c15feae8610372bcd,
title = {Learning from Between-class Examples for Deep Sound Recognition},
year = {2017},
url = {https://www.semanticscholar.org/paper/2c5174a4f3f6978cc28e2d3c15feae8610372bcd},
abstract = {Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.},
author = {Yuji Tokozume and Y. Ushiku and T. Harada},
journal = {ArXiv},
volume = {abs/1711.10282},
pages = {null},
arxivid = {1711.10282},
}

@article{0e39e519471cc41b232381bd529542e2c02f21fa,
title = {Environmental sound classification with convolutional neural networks},
year = {2015},
url = {https://www.semanticscholar.org/paper/0e39e519471cc41b232381bd529542e2c02f21fa},
abstract = {This paper evaluates the potential of convolutional neural networks in classifying short audio clips of environmental sounds. A deep model consisting of 2 convolutional layers with max-pooling and 2 fully connected layers is trained on a low level representation of audio data (segmented spectrograms) with deltas. The accuracy of the network is evaluated on 3 public datasets of environmental and urban recordings. The model outperforms baseline implementations relying on mel-frequency cepstral coefficients and achieves results comparable to other state-of-the-art approaches.},
author = {Karol J. Piczak},
journal = {2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)},
volume = {null},
pages = {1-6},
doi = {10.1109/MLSP.2015.7324337},
}

@article{51105fd0d833a9ffe219c1e7f5e8718849f6ba9f,
title = {Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes},
year = {2017},
url = {https://www.semanticscholar.org/paper/51105fd0d833a9ffe219c1e7f5e8718849f6ba9f},
abstract = {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and sets state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset using balanced training set.},
author = {Anurag Kumar and Maksim Khadkevich and C. Fügen},
journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {326-330},
doi = {10.1109/ICASSP.2018.8462200},
arxivid = {1711.01369},
}

@article{1deda3a5d996dffbd4ed0e97b994feb47e04217c,
title = {Environment Sound Classification using Multiple Feature Channels and Deep Convolutional Neural Networks},
year = {2019},
url = {https://www.semanticscholar.org/paper/1deda3a5d996dffbd4ed0e97b994feb47e04217c},
abstract = {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN). The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. Also, we employ a deeper CNN (DCNN) compared to previous models, consisting of 2D separable convolutions working on time and feature domain separately. The model also consists of max pooling layers that downsample time and feature domain separately. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (98.60%), ESC-10 (97.25%) and ESC-50 (95.50%). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets and by a considerable margin over the previous models. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7% and 81.3% respectively.},
author = {Jivitesh Sharma and Ole-Christoffer Granmo and M. G. Olsen},
journal = {ArXiv},
volume = {abs/1908.11219},
pages = {null},
arxivid = {1908.11219},
}

@article{13f0ce1b22b5058ed8a5e5e40e31e815c0b30464,
title = {URBANSOUND CLASSIFICATION},
year = {2020},
url = {https://www.semanticscholar.org/paper/13f0ce1b22b5058ed8a5e5e40e31e815c0b30464},
abstract = {In this paper, we propose to explore several deep machine learning methods for Urban sound classiﬁcation (USC) tasks. Our network architecture extracts high-level feature representations from spectrogram-like features (MFCC and Mel spectrogram). Furthermore, we test traditional machine learning methods and compare their performance with deep learning models. Experiments are conducted on UrbanSound8K. Our experimental results demonstrate that deep learning module (ResNet) has achieved the best performance.},
author = {H. Zhao and Tianyu Zhao},
}

@article{e6556a463c21cc3b114ac63b02b1667a73ff7d2c,
title = {Learning Environmental Sounds with Multi-scale Convolutional Neural Network},
year = {2018},
url = {https://www.semanticscholar.org/paper/e6556a463c21cc3b114ac63b02b1667a73ff7d2c},
abstract = {Deep learning has dramatically improved the performance of sounds recognition. However, learning acoustic models directly from the raw waveform is still challenging. Current waveform-based models generally use time-domain convolutional layers to extract features. The features extracted by single size filters are insufficient for building discriminative representation of audios. In this paper, we propose multi-scale convolution operation, which can get better audio representation by improving the frequency resolution and learning filters cross all frequency area. For leveraging the waveform-based features and spectrogram-based features in a single model, we introduce twophase method to fuse the different features. Finally, we propose a novel end-to-end network called WaveMsNet based on the multi-scale convolution operation and two-phase method. On the environmental sounds classification datasets ESC-10 and ESC-50, the classification accuracies of our WaveMsNet achieve 93.75% and 79.10% respectively, which improve significantly from the previous methods.},
author = {Boqing Zhu and Changjian Wang and Feng Liu and Jin Lei and Zengquan Lu and Yuxing Peng},
journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
volume = {null},
pages = {1-8},
doi = {10.1109/IJCNN.2018.8489641},
arxivid = {1803.10219},
}

@article{2aeeec17925ad1249b4f1939fd1f342fcf9103e4,
title = {Environment Sound Classification Using Multiple Feature Channels and Attention Based Deep Convolutional Neural Network},
year = {2019},
url = {https://www.semanticscholar.org/paper/2aeeec17925ad1249b4f1939fd1f342fcf9103e4},
abstract = {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN) with Attention mechanism. The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. And, we employ a deeper CNN (DCNN) compared to previous models, consisting of spatially separable convolutions working on time and feature domain separately. Alongside, we use attention modules that perform channel and spatial attention together. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (97.52%), ESC-10 (95.75%) and ESC-50 (88.50%). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7% and 81.3% respectively.},
author = {Jivitesh Sharma and Ole-Christoffer Granmo and Morten Goodwin},
doi = {10.21437/interspeech.2020-1303},
}

@article{8821c470aa6653a68f534b10818ba1b1b4298646,
title = {Learning Frame Level Attention for Environmental Sound Classification},
year = {2020},
url = {https://www.semanticscholar.org/paper/8821c470aa6653a68f534b10818ba1b1b4298646},
abstract = {Environmental sound classification (ESC) is a challenging problem due to the complexity of sounds. The classification performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. However, ESC often suffers from the semantically irrelevant frames and silent frames. In order to deal with this, we employ a frame-level attention model to focus on the semantically relevant frames and salient frames. Specifically, we first propose a convolutional recurrent neural network to learn spectro-temporal features and temporal correlations. Then, we extend our convolutional RNN model with a frame-level attention mechanism to learn discriminative feature representations for ESC. We investigated the classification performance when using different attention scaling function and applying different layers. Experiments were conducted on ESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness of the proposed method and our method achieved the state-of-the-art or competitive classification accuracy with lower computational complexity. We also visualized our attention results and observed that the proposed attention mechanism was able to lead the network tofocus on the semantically relevant parts of environmental sounds.},
author = {Zhichao Zhang and Shugong Xu and Shunqing Zhang and Tianhao Qiao and Shan Cao},
journal = {ArXiv},
volume = {abs/2007.07241},
pages = {null},
arxivid = {2007.07241},
}

@article{8af619469c676870000bf375da3121234a7abbef,
title = {Classifying environmental sounds using image recognition networks},
year = {2017},
url = {https://www.semanticscholar.org/paper/8af619469c676870000bf375da3121234a7abbef},
abstract = {S2 TL;DR: This paper considers the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds, and evaluates the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition.},
author = {Venkatesh Boddapati and Andrej Petef and J. Rasmusson and L. Lundberg},
doi = {10.1016/j.procs.2017.08.250},
}

@article{9b50f12e4dafb6532553d5043ed5cdf8d0426f5f,
title = {End-to-End Train Horn Detection for Railway Transit Safety},
year = {2022},
url = {https://www.semanticscholar.org/paper/9b50f12e4dafb6532553d5043ed5cdf8d0426f5f},
abstract = {The train horn sound is an active audible warning signal used for warning commuters and railway employees of the oncoming train(s), assuring a smooth operation and traffic safety, especially at barrier-free crossings. This work studies deep learning-based approaches to develop a system providing the early detection of train arrival based on the recognition of train horn sounds from the traffic soundscape. A custom dataset of train horn sounds, car horn sounds, and traffic noises is developed to conduct experiments and analysis. We propose a novel two-stream end-to-end CNN model (i.e., THD-RawNet), which combines two approaches of feature extraction from raw audio waveforms, for audio classification in train horn detection (THD). Besides a stream with a sequential one-dimensional CNN (1D-CNN) as in existing sound classification works, we propose to utilize multiple 1D-CNN branches to process raw waves in different temporal resolutions to extract an image-like representation for the 2D-CNN classification part. Our experiment results and comparative analysis have proved the effectiveness of the proposed two-stream network and the method of combining features extracted in multiple temporal resolutions. The THD-RawNet obtained better accuracies and robustness compared to those of baseline models trained on either raw audio or handcrafted features, in which at the input size of one second the network yielded an accuracy of 95.11% for testing data in normal traffic conditions and remained above a 93% accuracy for the considerable noisy condition of-10 dB SNR. The proposed THD system can be integrated into the smart railway crossing systems, private cars, and self-driving cars to improve railway transit safety.},
author = {van-Thuan Tran and Wei-Ho Tsai and Yury Furletov and M. Gorodnichev},
journal = {Sensors (Basel, Switzerland)},
volume = {22},
pages = {null},
doi = {10.3390/s22124453},
pmid = {35746234},
}

@article{99e6f700d374e34c8376f1f43af994b278924f28,
title = {ESC: Dataset for Environmental Sound Classification},
year = {2015},
url = {https://www.semanticscholar.org/paper/99e6f700d374e34c8376f1f43af994b278924f28},
abstract = {One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.},
author = {Karol J. Piczak},
journal = {Proceedings of the 23rd ACM international conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/2733373.2806390},
}

@article{94bbf0223b4c7dcaa0ff4f9a52c8558591d2f611,
title = {An Ensemble Stacked Convolutional Neural Network Model for Environmental Event Sound Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/94bbf0223b4c7dcaa0ff4f9a52c8558591d2f611},
abstract = {Convolutional neural networks (CNNs) with log-mel audio representation and CNN-based end-to-end learning have both been used for environmental event sound recognition (ESC). However, log-mel features can be complemented by features learned from the raw audio waveform with an effective fusion method. In this paper, we first propose a novel stacked CNN model with multiple convolutional layers of decreasing filter sizes to improve the performance of CNN models with either log-mel feature input or raw waveform input. These two models are then combined using the Dempster–Shafer (DS) evidence theory to build the ensemble DS-CNN model for ESC. Our experiments over three public datasets showed that our method could achieve much higher performance in environmental sound recognition than other CNN models with the same types of input features. This is achieved by exploiting the complementarity of the model based on log-mel feature input and the model based on learning features directly from raw waveforms.},
author = {Shaobo Li and Yong Yao and Jie Hu and Guokai Liu and Xuemei Yao and Jianjun Hu},
journal = {Applied Sciences},
volume = {null},
pages = {null},
doi = {10.3390/APP8071152},
}

@article{1f5066018662b7c7d13a57611e6f118b2871d39f,
title = {Data Augmentation Using Random Image Cropping and Patching for Deep CNNs},
year = {2018},
url = {https://www.semanticscholar.org/paper/1f5066018662b7c7d13a57611e6f118b2871d39f},
abstract = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage of the soft labels. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of 2.19% on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet, an image-caption retrieval task using Microsoft COCO, and other computer vision tasks.},
author = {Ryo Takahashi and Takashi Matsubara and K. Uehara},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
volume = {30},
pages = {2917-2931},
doi = {10.1109/TCSVT.2019.2935128},
arxivid = {1811.09030},
}

@article{1c88ca7a13b14e6afac1910bca866609254e92cb,
title = {End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network},
year = {2019},
url = {https://www.semanticscholar.org/paper/1c88ca7a13b14e6afac1910bca866609254e92cb},
abstract = {S2 TL;DR: An end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network that learns a representation directly from the audio signal that outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input.},
author = {Sajjad Abdoli and P. Cardinal and Alessandro Lameiras Koerich},
journal = {ArXiv},
volume = {abs/1904.08990},
pages = {null},
doi = {10.1016/J.ESWA.2019.06.040},
arxivid = {1904.08990},
}

@article{5bdf07c9897ca70788fff61dec56178a2bd0c29c,
title = {Deep Pyramidal Residual Networks},
year = {2016},
url = {https://www.semanticscholar.org/paper/5bdf07c9897ca70788fff61dec56178a2bd0c29c},
abstract = {Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet.},
author = {Dongyoon Han and Jiwhan Kim and Junmo Kim},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {6307-6315},
doi = {10.1109/CVPR.2017.668},
arxivid = {1610.02915},
}

@article{d3eef9744324abc397c82b112b026aad3ec56708,
title = {MixUp as Locally Linear Out-Of-Manifold Regularization},
year = {2018},
url = {https://www.semanticscholar.org/paper/d3eef9744324abc397c82b112b026aad3ec56708},
abstract = {MixUp (Zhang et al. 2017) is a recently proposed dataaugmentation scheme, which linearly interpolates a random pair of training examples and correspondingly the one-hot representations of their labels. Training deep neural networks with such additional data is shown capable of significantly improving the predictive accuracy of the current art. The power of MixUp, however, is primarily established empirically and its working and effectiveness have not been explained in any depth. In this paper, we develop an understanding for MixUp as a form of “out-of-manifold regularization”, which imposes certain “local linearity” constraints on the model’s input space beyond the data manifold. This analysis enables us to identify a limitation of MixUp, which we call “manifold intrusion”. In a nutshell, manifold intrusion in MixUp is a form of under-fitting resulting from conflicts between the synthetic labels of the mixed-up examples and the labels of original training data. Such a phenomenon usually happens when the parameters controlling the generation of mixing policies are not sufficiently fine-tuned on the training data. To address this issue, we propose a novel adaptive version of MixUp, where the mixing policies are automatically learned from the data using an additional network and objective function designed to avoid manifold intrusion. The proposed regularizer, AdaMixUp, is empirically evaluated on several benchmark datasets. Extensive experiments demonstrate that AdaMixUp improves upon MixUp when applied to the current art of deep classification models.},
author = {Hongyu Guo and Yongyi Mao and Richong Zhang},
journal = {ArXiv},
volume = {abs/1809.02499},
pages = {null},
doi = {10.1609/AAAI.V33I01.33013714},
arxivid = {1809.02499},
}

@article{b47b15834b8ccf7fcbc733dfae105f0f2d97b199,
title = {ESResNet: Environmental Sound Classification Based on Visual Domain Models},
year = {2020},
url = {https://www.semanticscholar.org/paper/b47b15834b8ccf7fcbc733dfae105f0f2d97b199},
abstract = {Environmental Sound Classification (ESC) is an active research area in the audio domain and has seen a lot of progress in the past years. However, many of the existing approaches achieve high accuracy by relying on domain-specific features and architectures, making it harder to benefit from advances in other fields (e.g., the image domain). Additionally, some of the past successes have been attributed to a discrepancy of how results are evaluated (i.e., on unofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall progression of the field. The contribution of this paper is twofold. First, we present a model that is inherently compatible with mono and stereo sound inputs. Our model is based on simple log-power Short-Time Fourier Transform (STFT) spectrograms and combines them with several well-known approaches from the image domain (i.e., ResNet, Siamese-like networks and attention). We investigate the influence of cross-domain pre-training, architectural changes, and evaluate our model on standard datasets. We find that our model out-performs all previously known approaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50) and 84.2 % / 85.4 % (US8K mono / stereo). Second, we provide a comprehensive overview of the actual state of the field, by differentiating several previously reported results on the US8K dataset between official or unofficial splits. For better reproducibility, our code (including any re- implementations) is made available.},
author = {A. Guzhov and Federico Raue and Jörn Hees and A. Dengel},
journal = {2020 25th International Conference on Pattern Recognition (ICPR)},
volume = {null},
pages = {4933-4940},
doi = {10.1109/ICPR48806.2021.9413035},
arxivid = {2004.07301},
}

@article{f427280ac6f5ca9c392222a0b7a733e2f09e3b97,
title = {Environmental Sound Classification Base on CNN and LightGBM},
year = {2019},
url = {https://www.semanticscholar.org/paper/f427280ac6f5ca9c392222a0b7a733e2f09e3b97},
abstract = {Aiming at the problem that the traditional convolutional neural network has insufficient generalization ability and low accuracy in environmental sound classification, a new model mixing deep CNN with LightGBM is proposed. Based on the preprocessing of the Mel Frequency cepstral coefficient matrix on the audio file, the new model firstly uses the deep convolutional neural network to extract features. Then, combined with the efficient and accurate characters of LightGBM in classification prediction, the extracted features are imported into LightGBM for training. Thereby it achieves the purpose of improving classification accuracy. The results of the comparative experiments on the UrbanSound8K public dataset show that the new model improves the accuracy of 7.7% compared to the using a single-use convolutional neural network model.},
author = {威平 廖},
journal = {Computer Science and Application},
volume = {null},
pages = {null},
doi = {10.12677/csa.2019.910212},
}

@article{e89c6aa9b0cc6048192a823d847848828b9b54b6,
title = {End-To-End Auditory Object Recognition Via Inception Nucleus},
year = {2020},
url = {https://www.semanticscholar.org/paper/e89c6aa9b0cc6048192a823d847848828b9b54b6},
abstract = {Machine learning approaches to auditory object recognition are traditionally based on engineered features such as those derived from the spectrum or cepstrum. More recently, end- to-end classification systems in image and auditory recognition systems have been developed to learn features jointly with classification and result in improved classification accuracy. In this paper, we propose a novel end-to-end deep neural network to map the raw waveform inputs to sound class labels. Our network includes an "inception nucleus" that optimizes the size of convolutional filters on the fly that results in reducing engineering efforts dramatically. Classification results compared favorably against current state-of-the-art approaches, besting them by 10.4 percentage points on the Ur- bansound8k dataset. Analyses of learned representations revealed that filters in the earlier hidden layers learned waveletlike transforms to extract features that were informative for classification.},
author = {M. K. Ebrahimpour and Timothy M. Shea and Andreea Danielescu and D. Noelle and Christopher T. Kello},
journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {146-150},
doi = {10.1109/ICASSP40776.2020.9054725},
arxivid = {2005.12195},
}

@article{ffac573c42c3d6826c60cb76a101727cbc784822,
title = {Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/ffac573c42c3d6826c60cb76a101727cbc784822},
abstract = {S2 TL;DR: This work proposes an convolutional recurrent neural network model to learn spectro-temporal features and temporal correlations and extends this model with a frame-level attention mechanism to learn discriminative feature representations for environmental sound classification.},
author = {Zhichao Zhang and Shugong Xu and Tianhao Qiao and Shunqing Zhang and Shan Cao},
journal = {Neurocomputing},
volume = {453},
pages = {896-903},
doi = {10.1007/978-3-030-31654-9_23},
arxivid = {1907.02230},
}

@article{53dd0ffe03b227aeb3730644fd233b4185d2c7ec,
title = {Mixup-Based Acoustic Scene Classification Using Multi-Channel Convolutional Neural Network},
year = {2018},
url = {https://www.semanticscholar.org/paper/53dd0ffe03b227aeb3730644fd233b4185d2c7ec},
abstract = {S2 TL;DR: This paper explores the use of Multi-channel CNN for the classification task, which aims to extract features from different channels in an end-to-end manner, and explores the using of mixup method, which can provide higher prediction accuracy and robustness in contrast with previous models.},
author = {Kele Xu and Dawei Feng and Haibo Mi and Boqing Zhu and Dezhi Wang and Lilun Zhang and Hengxing Cai and S. Liu},
journal = {ArXiv},
volume = {abs/1805.07319},
pages = {null},
doi = {10.1007/978-3-030-00764-5_2},
arxivid = {1805.07319},
}

@article{d65aff9c32814a117cb0826e793592e251112806,
title = {Deep Convolutional Neural Network Combined with Concatenated Spectrogram for Environmental Sound Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/d65aff9c32814a117cb0826e793592e251112806},
abstract = {Environmental sound classification (ESC) is an important but challenging issue. In this paper, we propose a new deep convolutional neural network, which uses concatenated spectrogram as input features, for ESC task. This concatenated spectrogram feature we adopt can increase the richness of features compared with single spectrogram. It is generated by concatenating two regular spectrograms, the Log-Mel spectrogram and the Log-Gammatone spectrogram. The network we propose uses convolutional blocks to extract and derive high-level feature images from concatenated spectrogram, and each block is composed of three convolutional layers and a pooling layer. In order to keep depth of the network and reduce numbers of parameters, we use filter with a small receptive field in each convolutional layer. Besides, we use the average pooling to keep more information. Our method was tested on ESC-50 and UrbanSound8K and achieved classification accuracy of 83.8% and 80.3%, respectively. The experimental results show that the proposed method is suitable for ESC task.},
author = {Zhejian Chi and Ying Li and Cheng Chen},
journal = {2019 IEEE 7th International Conference on Computer Science and Network Technology (ICCSNT)},
volume = {null},
pages = {251-254},
doi = {10.1109/ICCSNT47585.2019.8962462},
}

@article{2ebcc3e5714819ba98462b689262e6f99f31481d,
title = {Learning Attentive Representations for Environmental Sound Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/2ebcc3e5714819ba98462b689262e6f99f31481d},
abstract = {Environmental sound classification (ESC) is a challenging problem due to the complex temporal structure and diverse energy modulation patterns of environmental sounds. In order to deal with the former, temporal attention mechanism is originally adopted to focus on the informative frames. However, no existing works pay attention to the latter problem. In this paper, we consider the role of convolution filters in detecting energy modulation patterns and propose a channel attention mechanism to focus on the semantically relevant channels generated by corresponding filters. Furthermore, we incorporate the temporal attention and channel attention to enhance the representative power of CNN via generating complementary information. In addition, to avoid possible overfitting caused by limited training data, we explore a data augmentation scheme that is other contribution in this paper. We evaluate our proposed method on three benchmark ESC datasets: ESC-10 and ESC-50 and DCASE2016. Experimental results show the effectiveness of proposed method and achieve the state-of-the-art or competitive results in terms of classification accuracy. Finally, we visualize our attention results and observe that the proposed attention mechanism is able to lead the network to focus on the semantically relevant parts of environmental sounds.},
author = {Zhichao Zhang and Shugong Xu and Shunqing Zhang and Tianhao Qiao and Shan Cao},
journal = {IEEE Access},
volume = {7},
pages = {130327-130339},
doi = {10.1109/ACCESS.2019.2939495},
}

@article{176664d7d409415eac712a69c8c4ece98f55aa10,
title = {Novel Phase Encoded Mel Filterbank Energies for Environmental Sound Classification},
year = {2017},
url = {https://www.semanticscholar.org/paper/176664d7d409415eac712a69c8c4ece98f55aa10},
abstract = {S2 TL;DR: This paper proposes to use phase encoded filterbank energies (PEFBEs) for Environment Sound Classification task, and uses Convolutional Neural Network (CNN) as a pattern classifier for feature set.},
author = {Rishabh Tak and Dharmesh M. Agrawal and H. Patil},
doi = {10.1007/978-3-319-69900-4_40},
}

@article{bfdb193e4f93d728c6c4561a5f0c2a01aa1b34ec,
title = {CNN-Based Learnable Gammatone Filterbank and Equal-Loudness Normalization for Environmental Sound Classification},
year = {2020},
url = {https://www.semanticscholar.org/paper/bfdb193e4f93d728c6c4561a5f0c2a01aa1b34ec},
abstract = {For environmental sound classification (ESC), this letter presents a learnable auditory filterbank based on a one-dimensional (1D) convolutional neural network with strong psychophysiological inductive bias in the form of a gammatone filterbank and an equal-loudness prompting normalization. In the past, a number of ESC methods based on learnable auditory features obtained by performing plain 1D convolutions on raw input waveforms for outperforming traditional handcrafted features such as a mel-frequency filterbank have been proposed. However, the large number of parameters involved in the convolutions suggests that these methods will not generalize better than a model defined by a smaller number of parameters, which is considered in this letter. Here, a learnable gammatone filterbank layer consisting of 1D kernels represented by a parametric form of the bandpass gammatone filters is proposed for acquiring a time-frequency representation of the raw waveform. A normalization with learnable parameters that control the trade-off between energy equalization and structure preservation in the spectro-temporal domain is proposed. To verify the effectiveness of the considered network and the normalization, ESC experiments on the ESC-50 and UrbanSound8K datasets were conducted. Compared to other state-of-the-art networks, the considered network performed better on the two datasets. In addition, an ensemble architecture achieved further performance improvement.},
author = {Hyunsin Park and C. Yoo},
journal = {IEEE Signal Processing Letters},
volume = {27},
pages = {411-415},
doi = {10.1109/LSP.2020.2975422},
}

@article{dfe95b52d439fe86cca663f8815fd4cfb7cb4728,
title = {Environment Sound Classification Using a Two-Stream CNN Based on Decision-Level Fusion},
year = {2019},
url = {https://www.semanticscholar.org/paper/dfe95b52d439fe86cca663f8815fd4cfb7cb4728},
abstract = {With the popularity of using deep learning-based models in various categorization problems and their proven robustness compared to conventional methods, a growing number of researchers have exploited such methods in environment sound classification tasks in recent years. However, the performances of existing models use auditory features like log-mel spectrogram (LM) and mel frequency cepstral coefficient (MFCC), or raw waveform to train deep neural networks for environment sound classification (ESC) are unsatisfactory. In this paper, we first propose two combined features to give a more comprehensive representation of environment sounds Then, a fourfour-layer convolutional neural network (CNN) is presented to improve the performance of ESC with the proposed aggregated features. Finally, the CNN trained with different features are fused using the Dempster–Shafer evidence theory to compose TSCNN-DS model. The experiment results indicate that our combined features with the four-layer CNN are appropriate for environment sound taxonomic problems and dramatically outperform other conventional methods. The proposed TSCNN-DS model achieves a classification accuracy of 97.2%, which is the highest taxonomic accuracy on UrbanSound8K datasets compared to existing models.},
author = {Yu Su and Ke Zhang and Jingyu Wang and K. Madani},
journal = {Sensors (Basel, Switzerland)},
volume = {19},
pages = {null},
doi = {10.3390/s19071733},
pmid = {30978974},
}

@article{dddada8efb06136777983942c360bf3b76d5ea5f,
title = {Improved Mixed-Example Data Augmentation},
year = {2018},
url = {https://www.semanticscholar.org/paper/dddada8efb06136777983942c360bf3b76d5ea5f},
abstract = {In order to reduce overfitting, neural networks are typically trained with data augmentation, the practice of artificially generating additional training data via label-preserving transformations of existing training examples. While these types of transformations make intuitive sense, recent work has demonstrated that even non-label-preserving data augmentation can be surprisingly effective, examining this type of data augmentation through linear combinations of pairs of examples. Despite their effectiveness, little is known about why such methods work. In this work, we aim to explore a new, more generalized form of this type of data augmentation in order to determine whether such linearity is necessary. By considering this broader scope of "mixed-example data augmentation", we find a much larger space of practical augmentation techniques, including methods that improve upon previous state-of-the-art. This generalization has benefits beyond the promise of improved performance, revealing a number of types of mixed-example data augmentation that are radically different from those considered in prior work, which provides evidence that current theories for the effectiveness of such methods are incomplete and suggests that any such theory must explain a much broader phenomenon.},
author = {Cecilia Summers and M. Dinneen},
journal = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
volume = {null},
pages = {1262-1270},
doi = {10.1109/WACV.2019.00139},
arxivid = {1805.11272},
}

@article{59d8c68de09da69a608ceb149f40114f5538c5b1,
title = {CNN architectures for large-scale audio classification},
year = {2016},
url = {https://www.semanticscholar.org/paper/59d8c68de09da69a608ceb149f40114f5538c5b1},
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
author = {Shawn Hershey and Sourish Chaudhuri and D. Ellis and J. Gemmeke and A. Jansen and R. C. Moore and Manoj Plakal and D. Platt and R. Saurous and Bryan Seybold and M. Slaney and Ron J. Weiss and K. Wilson},
journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {131-135},
doi = {10.1109/ICASSP.2017.7952132},
arxivid = {1609.09430},
}

@article{9be365717c8c487629370f80e68a9d8df8737a4a,
title = {Multi-channel Convolutional Neural Networks with Multi-level Feature Fusion for Environmental Sound Classification},
year = {2018},
url = {https://www.semanticscholar.org/paper/9be365717c8c487629370f80e68a9d8df8737a4a},
abstract = {S2 TL;DR: The proposed method outperforms the state-of-the-art end-to-end methods for environmental sound classification in terms of the classification accuracy and is Inspired by VGG networks.},
author = {Dading Chong and Yuexian Zou and Wenwu Wang},
doi = {10.1007/978-3-030-05716-9_13},
}

@article{43863461098a6c774bec74362d1f20e44dd41445,
title = {Learning environmental sounds with end-to-end convolutional neural network},
year = {2017},
url = {https://www.semanticscholar.org/paper/43863461098a6c774bec74362d1f20e44dd41445},
abstract = {Environmental sound classification (ESC) is usually conducted based on handcrafted features such as the log-mel feature. Meanwhile, end-to-end classification systems perform feature extraction jointly with classification and have achieved success particularly in image classification. In the same manner, if environmental sounds could be directly learned from the raw waveforms, we would be able to extract a new feature effective for classification that could not have been designed by humans, and this new feature could improve the classification performance. In this paper, we propose a novel end-to-end ESC system using a convolutional neural network (CNN). The classification accuracy of our system on ESC-50 is 5.1% higher than that achieved when using logmel-CNN with the static log-mel feature. Moreover, we achieve a 6.5% improvement in classification accuracy over the state-of-the-art logmel-CNN with the static and delta log-mel feature, simply by combining our system and logmel-CNN.},
author = {Yuji Tokozume and T. Harada},
journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {2721-2725},
doi = {10.1109/ICASSP.2017.7952651},
}

@article{22b6f5febae9ecab5f080fb94b4437951a337e12,
title = {Data Augmentation by Pairing Samples for Images Classification},
year = {2018},
url = {https://www.semanticscholar.org/paper/22b6f5febae9ecab5f080fb94b4437951a337e12},
abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate N^2 new samples from N training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
author = {H. Inoue},
journal = {ArXiv},
volume = {abs/1801.02929},
pages = {null},
arxivid = {1801.02929},
}

@article{b302be08cd43b26b202c6fda553f404742f30260,
title = {The Application and Improvement of Deep Neural Networks in Environmental Sound Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/b302be08cd43b26b202c6fda553f404742f30260},
abstract = {Neural networks have achieved great results in sound recognition, and many different kinds of acoustic features have been tried as the training input for the network. However, there is still doubt about whether a neural network can efficiently extract features from the raw audio signal input. This study improved the raw-signal-input network from other researches using deeper network architectures. The raw signals could be better analyzed in the proposed network. We also presented a discussion of several kinds of network settings, and with the spectrogram-like conversion, our network could reach an accuracy of 73.55% in the open-audio-dataset “Dataset for Environmental Sound Classification 50” (ESC50). This study also proposed a network architecture that could combine different kinds of network feeds with different features. With the help of global pooling, a flexible fusion way was integrated into the network. Our experiment successfully combined two different networks with different audio feature inputs (a raw audio signal and the log-mel spectrum). Using the above settings, the proposed ParallelNet finally reached the accuracy of 81.55% in ESC50, which also reached the recognition level of human beings.},
author = {Yu-Kai Lin and M. Su and Yi-Zeng Hsieh},
journal = {Applied Sciences},
volume = {null},
pages = {null},
doi = {10.3390/app10175965},
}

@article{24e1a9eabec4adb37a46eae5ff51198e6e758a44,
title = {Deep Convolutional Neural Network with Mixup for Environmental Sound Classification},
year = {2018},
url = {https://www.semanticscholar.org/paper/24e1a9eabec4adb37a46eae5ff51198e6e758a44},
abstract = {S2 TL;DR: A novel deep convolutional neural network is proposed to be used for environmental sound classification (ESC) tasks that uses stacked Convolutional and pooling layers to extract high-level feature representations from spectrogram-like features.},
author = {Zhichao Zhang and Shugong Xu and Shan Cao and Shunqing Zhang},
doi = {10.1007/978-3-030-03335-4_31},
arxivid = {1808.08405},
}

@article{e41bbabd84bfbce6ef66825c1a2d7eb869bd1202,
title = {Between-Class Learning for Image Classification},
year = {2017},
url = {https://www.semanticscholar.org/paper/e41bbabd84bfbce6ef66825c1a2d7eb869bd1202},
abstract = {In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning)1. We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.2},
author = {Yuji Tokozume and Y. Ushiku and T. Harada},
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
volume = {null},
pages = {5486-5494},
doi = {10.1109/CVPR.2018.00575},
arxivid = {1711.10284},
}

@article{cddd943dae3a6ca5c591d0c5be0bb5b490b28c63,
title = {Very deep convolutional neural networks for raw waveforms},
year = {2016},
url = {https://www.semanticscholar.org/paper/cddd943dae3a6ca5c591d0c5be0bb5b490b28c63},
abstract = {Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (∼2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperforms the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and is competitive with the performance of models using log-mel features.},
author = {Wei Dai and Chia Dai and Shuhui Qu and Juncheng Billy Li and Samarjit Das},
journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {421-425},
doi = {10.1109/ICASSP.2017.7952190},
arxivid = {1610.00087},
}

@article{29df280ff54d7478fee6a626fbb9ca1b21234287,
title = {Multi-stream Network With Temporal Attention For Environmental Sound Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/29df280ff54d7478fee6a626fbb9ca1b21234287},
abstract = {Environmental sound classification systems often do not perform robustly across different sound classification tasks and audio signals of varying temporal structures. We introduce a multi-stream convolutional neural network with temporal attention that addresses these problems. The network relies on three input streams consisting of raw audio and spectral features and utilizes a temporal attention function computed from energy changes over time. Training and classification utilizes decision fusion and data augmentation techniques that incorporate uncertainty. We evaluate this network on three commonly used data sets for environmental sound and audio scene classification and achieve new state-of-the-art performance without any changes in network architecture or front-end preprocessing, thus demonstrating better generalizability.},
author = {Xinyu Li and Venkata Chebiyyam and K. Kirchhoff},
doi = {10.21437/interspeech.2019-3019},
arxivid = {1901.08608},
}

@article{a18c444a72a10c463453259940da26910e61a59f,
title = {Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification},
year = {2016},
url = {https://www.semanticscholar.org/paper/a18c444a72a10c463453259940da26910e61a59f},
abstract = {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a “shallow” dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
author = {J. Salamon and J. Bello},
journal = {IEEE Signal Processing Letters},
volume = {24},
pages = {279-283},
doi = {10.1109/LSP.2017.2657381},
arxivid = {1608.04363},
}

@article{f6fd1be38a2d764d900b11b382a379efe88b3ed6,
title = {Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification},
year = {2017},
url = {https://www.semanticscholar.org/paper/f6fd1be38a2d764d900b11b382a379efe88b3ed6},
abstract = {In this paper, we propose to use Convolutional Restricted Boltzmann Machine (ConvRBM) to learn filterbank from the raw audio signals. ConvRBM is a generative model trained in an unsupervised way to model the audio signals of arbitrary lengths. ConvRBM is trained using annealed dropout technique and parameters are optimized using Adam optimization. The subband filters of ConvRBM learned from the ESC-50 database resemble Fourier basis in the mid-frequency range while some of the low-frequency subband filters resemble Gammatone basis. The auditory-like filterbank scale is nonlinear w.r.t. the center frequencies of the subband filters and follows the standard auditory scales. We have used our proposed model as a front-end for the Environmental Sound Classification (ESC) task with supervised Convolutional Neural Network (CNN) as a back-end. Using CNN classifier, the ConvRBM filterbank (ConvRBMBANK) and its score-level fusion with the Mel filterbank energies (FBEs) gave an absolute improvement of 10.65 %, and 18.70 % in the classification accuracy, respectively, over FBEs alone on the ESC-50 database. This shows that the proposed ConvRBM filterbank also contains highly complementary information over the Mel filterbank, which is helpful in the ESC task.},
author = {Hardik B. Sailor and Dharmesh M. Agrawal and H. Patil},
doi = {10.21437/Interspeech.2017-831},
}

@article{b676463dcba380aba56978fd6d68994aa7c4b58c,
title = {What Affects the Performance of Convolutional Neural Networks for Audio Event Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/b676463dcba380aba56978fd6d68994aa7c4b58c},
abstract = {Convolutional neural networks (CNN) have played an important role in Audio Event Classification (AEC). Both 1D-CNN and 2D-CNN methods have been applied to improve the classification accuracy of AEC, and there are many factors affecting the performance of models based on CNN. In this paper, we study different factors affecting the performance of CNN for AEC, including sampling rate, signal segmentation methods, window size, mel bins and filter size. The segmentation method of the event signal is an important one among them. It may lead to overfitting problem because audio events usually happen only for a short duration. We propose a signal segmentation method called Fill-length Processing to address the problem. Based on our study of these factors, we design convolutional neural networks for audio event classification (called FPNet). On the environmental sounds dataset ESC-50, the classification accuracies of FPNet-1D and FPNet-2D achieve 73.90% and 85.10% respectively, which improve significantly comparing to the previous methods.},
author = {Helin Wang and Dading Chong and Dongyan Huang and Yuexian Zou},
journal = {2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)},
volume = {null},
pages = {140-146},
doi = {10.1109/ACIIW.2019.8925277},
}

@article{65bf88e8485484dc85b1d656bd52b3a349aec705,
title = {Utilizing Domain Knowledge in End-to-End Audio Processing},
year = {2017},
url = {https://www.semanticscholar.org/paper/65bf88e8485484dc85b1d656bd52b3a349aec705},
abstract = {End-to-end neural network based approaches to audio modelling are generally outperformed by models trained on high-level data representations. In this paper we present preliminary work that shows the feasibility of training the first layers of a deep convolutional neural network (CNN) model to learn the commonly-used log-scaled mel-spectrogram transformation. Secondly, we demonstrate that upon initializing the first layers of an end-to-end CNN classifier with the learned transformation, convergence and performance on the ESC-50 environmental sound classification dataset are similar to a CNN-based model trained on the highly pre-processed log-scaled mel-spectrogram features.},
author = {T. M. S. Tax and J. Antich and Hendrik Purwins and Lars Maaløe},
journal = {ArXiv},
volume = {abs/1712.00254},
pages = {null},
arxivid = {1712.00254},
}

@article{9d56191692164f70319bb763db316d0f407d6565,
title = {SwGridNet: A Deep Convolutional Neural Network based on Grid Topology for Image Classification},
year = {2017},
url = {https://www.semanticscholar.org/paper/9d56191692164f70319bb763db316d0f407d6565},
abstract = {Deep convolutional neural networks (CNNs) achieve remarkable performance on image classification tasks. Recent studies, however, have demonstrated that generalization abilities are more important than the depth of neural networks for improving performance on image classification tasks. Herein, a new neural network called SwGridNet is proposed. A SwGridNet includes many convolutional processing units which connect mutually as a grid network where many processing paths exist between input and output. A SwGridNet has high generalization capability because the multipath architecture has the same effect of ensemble learning. As described in this paper, details of the SwGridNet network architecture are presented. Experimentally obtained results presented in this paper show that SwGridNets respectively achieve test error rates of 2.95% and 15.67% in a CIFAR-10 and CIFAR-100 classification tasks. The results indicate that the SwGridNet performance approximates that of state-of-the-art deep CNNs.},
author = {Atsushi Takeda},
journal = {ArXiv},
volume = {abs/1709.07646},
pages = {null},
arxivid = {1709.07646},
}
