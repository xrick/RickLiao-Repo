@article{abd1c342495432171beb7ca8fd9551ef13cbd0ff,
title = {ImageNet classification with deep convolutional neural networks},
year = {2012},
url = {https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff},
abstract = {null},
author = {A. Krizhevsky and I. Sutskever and Geoffrey E. Hinton},
journal = {Communications of the ACM},
volume = {60},
pages = {84 - 90},
doi = {10.1145/3065386},
}

@article{995c5f5e62614fcb4d2796ad2faab969da51713e,
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
url = {https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
author = {Sergey Ioffe and Christian Szegedy},
journal = {ArXiv},
volume = {abs/1502.03167},
pages = {null},
arxivid = {1502.03167},
}

@article{39a7a74de74efac3b8123650315d55cfb9d5220c,
title = {A Dataset and Taxonomy for Urban Sound Research},
year = {2014},
url = {https://www.semanticscholar.org/paper/39a7a74de74efac3b8123650315d55cfb9d5220c},
abstract = {Automatic urban sound classification is a growing area of research with applications in multimedia retrieval and urban informatics. In this paper we identify two main barriers to research in this area - the lack of a common taxonomy and the scarceness of large, real-world, annotated data. To address these issues we present a taxonomy of urban sounds and a new dataset, UrbanSound, containing 27 hours of audio with 18.5 hours of annotated sound event occurrences across 10 sound classes. The challenges presented by the new dataset are studied through a series of experiments using a baseline classification system.},
author = {J. Salamon and C. Jacoby and J. Bello},
journal = {Proceedings of the 22nd ACM international conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/2647868.2655045},
}

@article{eb42cf88027de515750f230b23b1a057dc782108,
title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
year = {2014},
url = {https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108},
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
author = {K. Simonyan and Andrew Zisserman},
journal = {CoRR},
volume = {abs/1409.1556},
pages = {null},
arxivid = {1409.1556},
}

@article{7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9,
title = {SoundNet: Learning Sound Representations from Unlabeled Video},
year = {2016},
url = {https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9},
abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
author = {Y. Aytar and Carl Vondrick and A. Torralba},
journal = {ArXiv},
volume = {abs/1610.09001},
pages = {null},
arxivid = {1610.09001},
}

@article{a6cb366736791bcccc5c8639de5a8f9636bf87e8,
title = {Adam: A Method for Stochastic Optimization},
year = {2014},
url = {https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8},
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
author = {Diederik P. Kingma and Jimmy Ba},
journal = {CoRR},
volume = {abs/1412.6980},
pages = {null},
arxivid = {1412.6980},
}

@article{2c03df8b48bf3fa39054345bafabfeff15bfd11d,
title = {Deep Residual Learning for Image Recognition},
year = {2015},
url = {https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d},
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {770-778},
doi = {10.1109/cvpr.2016.90},
arxivid = {1512.03385},
}

@article{34f25a8704614163c4095b3ee2fc969b60de4698,
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
url = {https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Nitish Srivastava and Geoffrey E. Hinton and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},
journal = {J. Mach. Learn. Res.},
volume = {15},
pages = {1929-1958},
doi = {10.5555/2627435.2670313},
}

@article{da870b1d61728c8f99263220824b22b67e1a7560,
title = {Sound Classification in a Smart Room Environment: an Approach using GMM and HMM Methods},
year = {2007},
url = {https://www.semanticscholar.org/paper/da870b1d61728c8f99263220824b22b67e1a7560},
abstract = {Because of cost or convenience reasons, patients or elderly people would be hospitalized at home and smart information systems would be needed in order to assist human operators. In this case, position and physiologic sensors give already numerous informations, but there are few studies for sound use in patient's habitation. However, sound classification and speech recognition may greatly increase the versatility of such a system: this will be provided by detecting short sentences or words which could characterize a distress situation for the patient. Analysis and classification of sounds emitted in patient's habitation may be useful for patient's activity monitoring. GMMs and HMMs are well suited for sound classification. Until now, GMMs are frequently used for sound classification in smart rooms because of their low computational costs, but HMMs should allow a finer analysis: indeed the use of 3 states HMMs should allow better performances by taking into account the variation of the signal according to time. For this framework a new sound corpus was recorded in experimental conditions. This corpus includes 8 sound classes useful for our application. The choice of needed acoustical features and the two approaches are presented. Then an evaluation is made with the initial corpus and with additional experimental noise. The obtained results are compared. At the end of this framework a segmentation module is presented. This module has the ability of extracting isolated sounds in a record by the means of a wavelet filtering method which allows the extraction in noisy conditions.},
author = {Michel Vacher and J. Serignat and Stéphane Chaillol},
}

@article{0ae21e1bddb58e31ede3fc036ec63c8f79bd6533,
title = {Acoustic Scene Classification: Classifying environments from the sounds they produce},
year = {2014},
url = {https://www.semanticscholar.org/paper/0ae21e1bddb58e31ede3fc036ec63c8f79bd6533},
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {D. Barchiesi and D. Giannoulis and D. Stowell and Mark D. Plumbley},
journal = {IEEE Signal Processing Magazine},
volume = {32},
pages = {16-34},
doi = {10.1109/MSP.2014.2326181},
arxivid = {1411.3715},
}
