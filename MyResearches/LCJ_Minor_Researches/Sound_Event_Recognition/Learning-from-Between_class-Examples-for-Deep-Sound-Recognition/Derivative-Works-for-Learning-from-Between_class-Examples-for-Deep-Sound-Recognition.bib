@article{71dbbb20b0d633abd6c27fb9499e8dee0261fd73,
title = {Environmental Sound Classification With Low-Complexity Convolutional Neural Network Empowered by Sparse Salient Region Pooling},
year = {2023},
url = {https://www.semanticscholar.org/paper/71dbbb20b0d633abd6c27fb9499e8dee0261fd73},
abstract = {Environmental Sound Classification (ESC) is an important field in a broad range of applications, such as smart cities, audio surveillance, and health care. Recently, Convolutional Neural Networks (CNNs) have taken the lead from traditional approaches and have produced promising results. However, the achieved improvements are often accompanied by increasing depth, complexity, and size of the network, which prevents their usage in many practical applications. In this work, our goal is to empower a small-size low-complexity CNN model to achieve superior performance. To this end, we concentrate on the importance of global pooling technique, which is less investigated in ESC. In most previous works, models utilize global average pooling layer which does not consider regional saliency, and thus weakens the salient time-frequency regions contributions to the classification, and also to the training of convolutional kernels. We propose a novel global pooling method, called Sparse Salient Region Pooling (SSRP), which computes the channel descriptors using a sparse subset of features, and guides the model to effectively learn from the more salient time-frequency regions. Experimental results demonstrate that the proposed model with only 700K parameters yields accuracies of 86.7% on ESC-50 and 94.8% on ESC-10, which are comparable to that of the state-of-the-art methods. Compared to the baseline model, our model achieves absolute improvement of 21.8% in accuracy on ESC-50, with 98% smaller model size. Our visual analyses show that SSRP intensifies the responses of low-energy regions such that they contribute even more than high-energy regions to the classification of specific sound classes.},
author = {Hamed Riazati Seresht and K. Mohammadi},
journal = {IEEE Access},
volume = {11},
pages = {849-862},
doi = {10.1109/ACCESS.2022.3232807},
}

@article{287d5efca8b2afe21627501d5b8b7d02033d8ac4,
title = {Attention Based Convolutional Neural Network with Multi-frequency Resolution Feature for Environment Sound Classification},
year = {2022},
url = {https://www.semanticscholar.org/paper/287d5efca8b2afe21627501d5b8b7d02033d8ac4},
abstract = {S2 TL;DR: A novel multi-frequency resolution (MFR) feature is proposed in this paper to solve the problem that the existing single frequency resolution timeâ€“frequency features of sound cannot effectively express the characteristics of multiple types of sound.},
author = {Minze Li and Wu Huang and Tao Zhang},
journal = {Neural Processing Letters},
volume = {null},
pages = {1 - 16},
doi = {10.1007/s11063-022-11041-y},
pmid = {36312843},
}

@article{47cd14e14d968d3dc399b6582aa77c4a464ce248,
title = {Environmental Sound Classification on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices},
year = {2021},
url = {https://www.semanticscholar.org/paper/47cd14e14d968d3dc399b6582aa77c4a464ce248},
abstract = {S2 TL;DR: This work presents a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices and describes a successful implementation on a standard off-the-shelf microcontroller and reports successful tests on real-world datasets.},
author = {Md Mohaimenuzzaman and C. Bergmeir and I. West and B. Meyer},
journal = {Pattern Recognit.},
volume = {133},
pages = {109025},
doi = {10.1016/j.patcog.2022.109025},
arxivid = {2103.03483},
}

@article{a35e8648f692397432604846bb428b3ea2c65d4e,
title = {Convolutional Neural Network-Gated Recurrent Unit Neural Network with Feature Fusion for Environmental Sound Classification},
year = {2021},
url = {https://www.semanticscholar.org/paper/a35e8648f692397432604846bb428b3ea2c65d4e},
abstract = {S2 TL;DR: A fusion of multiple features consisting of Log mel, log- scaled cochleagram and log-scaled constant-Q transform are proposed, and these features are fused to form the feature set that is called LMCC and a network called CNN-GRUNN is presented to improve the performance of ESC with the proposed aggregated features.},
author = {Yu Zhang and J. Zeng and Y. Li and Da Chen},
journal = {Automatic Control and Computer Sciences},
volume = {55},
pages = {311 - 318},
doi = {10.3103/S0146411621040106},
}

@article{7edbafd897124f12552e9d4743b398d7cf6e8518,
title = {Efficient Classification of Environmental Sounds through Multiple Features Aggregation and Data Enhancement Techniques for Spectrogram Images},
year = {2020},
url = {https://www.semanticscholar.org/paper/7edbafd897124f12552e9d4743b398d7cf6e8518},
abstract = {Over the past few years, the study of environmental sound classification (ESC) has become very popular due to the intricate nature of environmental sounds. This paper reports our study on employing various acoustic features aggregation and data enhancement approaches for the effective classification of environmental sounds. The proposed data augmentation techniques are mixtures of the reinforcement, aggregation, and combination of distinct acoustics features. These features are known as spectrogram image features (SIFs) and retrieved by different audio feature extraction techniques. All audio features used in this manuscript are categorized into two groups: one with general features and the other with Mel filter bank-based acoustic features. Two novel and innovative features based on the logarithmic scale of the Mel spectrogram (Mel), Log (Log-Mel) and Log (Log (Log-Mel)) denoted as L2M and L3M are introduced in this paper. In our study, three prevailing ESC benchmark datasets, ESC-10, ESC-50, and Urbansound8k (Us8k) are used. Most of the audio clips in these datasets are not fully acquired with sound and include silence parts. Therefore, silence trimming is implemented as one of the pre-processing techniques. The training is conducted by using the transfer learning model DenseNet-161, which is further fine-tuned with individual optimal learning rates based on the discriminative learning technique. The proposed methodologies attain state-of-the-art outcomes for all used ESC datasets, i.e., 99.22% for ESC-10, 98.52% for ESC-50, and 97.98% for Us8k. This work also considers real-time audio data to evaluate the performance and efficiency of the proposed techniques. The implemented approaches also have competitive results on real-time audio data.},
author = {Zohaib Mushtaq and S. Su},
journal = {Symmetry},
volume = {12},
pages = {1822},
doi = {10.3390/sym12111822},
}

@article{a9f90d601eba4c33ab4e7853164c2e688dca38f7,
title = {A Lightweight Channel and Time Attention Enhanced 1D CNN Model for Environmental Sound Classification},
year = {2024},
url = {https://www.semanticscholar.org/paper/a9f90d601eba4c33ab4e7853164c2e688dca38f7},
abstract = {null},
author = {Huaxing Xu and Yunzhi Tian and Haichuan Ren and Xudong Liu},
journal = {Expert Syst. Appl.},
volume = {249},
pages = {123768},
doi = {10.1016/j.eswa.2024.123768},
}

@article{dbd382fa0e8dc8a288a1d18b51d13dad5c8b597f,
title = {Environmental Sound Classification: A descriptive review of the literature},
year = {2022},
url = {https://www.semanticscholar.org/paper/dbd382fa0e8dc8a288a1d18b51d13dad5c8b597f},
abstract = {null},
author = {Anam Bansal and N. Garg},
journal = {Intell. Syst. Appl.},
volume = {16},
pages = {200115},
doi = {10.1016/j.iswa.2022.200115},
}

@article{4715f0c46096ce2cb0a41fe14e021a00c36c093e,
title = {Fast environmental sound classification based on resource adaptive convolutional neural network},
year = {2022},
url = {https://www.semanticscholar.org/paper/4715f0c46096ce2cb0a41fe14e021a00c36c093e},
abstract = {S2 TL;DR: A resource adaptive convolutional neural network (RACNN), which can generate the same number of feature maps as conventional convolution operations more cheaply, and extract the time and frequency features of audio efficiently, and achieves higher performance than the state-of-the-art methods with lower computational complexity.},
author = {Zhengzheng Fang and Bo Yin and Zehua Du and Xianqing Huang},
journal = {Scientific Reports},
volume = {12},
pages = {null},
doi = {10.1038/s41598-022-10382-x},
pmid = {35459273},
}

@article{77b102f9c63eefeb9fcb996d2af0030227cd6259,
title = {Recognize the surrounding: Development and evaluation of convolutional deep networks using gammatone spectrograms and raw audio signals},
year = {2022},
url = {https://www.semanticscholar.org/paper/77b102f9c63eefeb9fcb996d2af0030227cd6259},
abstract = {null},
author = {S. Gupta and Shifat Hossain and Ki-Doo Kim},
journal = {Expert Syst. Appl.},
volume = {200},
pages = {116998},
doi = {10.1016/j.eswa.2022.116998},
}

@article{ed7d43d14adc24076382aa6f7eb78ba533400b7a,
title = {Spectral images based environmental sound classification using CNN with meaningful data augmentation},
year = {2021},
url = {https://www.semanticscholar.org/paper/ed7d43d14adc24076382aa6f7eb78ba533400b7a},
abstract = {S2 TL;DR: The results show the effectiveness, robustness, and high accuracy of the proposed approach to have meaningful data augmentation by considering variations applied to the audio clips directly.},
author = {Zohaib Mushtaq and S. Su and Quoc-Viet Tran},
journal = {Applied Acoustics},
volume = {172},
pages = {107581},
doi = {10.1016/j.apacoust.2020.107581},
}
